# Alfred v0.3.0 - README

Alfred is a lightweight AI assistant project powered by a FastAPI backend, a React frontend, OpenAI GPT models for the main agent, and a LLaMA 3 local model running via Ollama for secondary agents.

## Demo

![Demo of feature](./assets/demo_v0.3.0.gif.gif)

## Features

- Web-based chat interface for interacting with Alfred in real time
- Short-term memory for smooth, context-aware conversations
- Long-term memory stored in a user-specific SQLite database for personalization and recall
- User-specific memory handling for both short-term and long-term contexts
- Detailed weather reports for any location via the OpenWeatherMap API
    _Uses OpenCageData for accurate geolocation_
- Uses GPT-4 via OpenAI for main agent reasoning, with local LLaMA3 for memory tools and context lookups

---

## ğŸ› ï¸ Setup Instructions

Make sure the following components are installed before running Alfred:

- Python 3.10+
- Node.js (v18 or later recommended) on your global machine
- [Ollama](https://ollama.com)
- `uv` or `pip` for managing Python packages
- `npm` or `pnpm` for managing frontend packages

---

## ğŸ”‘ Environment Variables

Set up your environment variables:

1. Rename `.env.example` to `.env` in both the backend and the frontend directories.
   These `.env` files will hold all of your secrets (like API keys), which the scripts will load automatically when needed.

2. Gather the necessary API keys (see instructions and placeholders inside each `.env.example`) and paste them into your `.env` files.

3. Fill in any optional configuration details you want to personalize Alfred with, such as DEFAULT_USER_ID, LOCATION, and FAMILY_NAME.

---

## ğŸš€ Running the Project

Run these commands in separate terminals or background processes.

1. **Start the backend server:**

    ```bash
    source venv/bin/activate  # Activate your virtual environment
    uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    ```

    - `0.0.0.0` allows access from other devices on the same network (if firewall/network permits).
    - Change `--port` if 8000 is already in use, and update your frontend `.env` accordingly.

2. **Install frontend dependencies (only once or after updating package.json):**

    ```bash
    cd frontend
    npm install
    ```

    - This will generate the `node_modules` directory with the dependencies required for the frontend.

3. **Start the frontend:**

    ```bash
    cd frontend
    npm run dev --host
    ```

    - Verify you are running this from the `frontend` directory.

4. **Start the LLaMA 3 model with Ollama:**

    ```bash
    ollama run llama3:8b
    ```

    - You can run this from any terminal; it doesnâ€™t need to be inside the project folder or virtual environment.
    - Make sure it's running before using Alfred, as it currently powers the chat history and memory functionality.

5. **Access the frontend:**

    - On the server machine: [http://localhost:5173](http://localhost:5173)  
    - On another device in the local network: `http://<server-ip-address>:5173`

---

## ğŸ“ Directory Structure (partial)

```plaintext
alfred_v0.1.3/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ .venv      # Python virtual environment (not committed)
â”‚   â”œâ”€â”€ nodes/     # Nodes (child agents)
â”‚   â”œâ”€â”€ utils/     # Tools and SQLite database utilities
â”‚   â”œâ”€â”€ .env       # Backend environment variables
â”‚   â”œâ”€â”€ alfred_memory.db       # Database (auto-generated)
â”‚   â”œâ”€â”€ main.py    # FastAPI backend entry point
â”‚   â””â”€â”€ requirements.txt       # Python dependencies
â”‚
â”œâ”€â”€ frontend/ # React frontend
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/       # Main UI source (/components/Chat.jsx)
â”‚   â”œâ”€â”€ .env       # Frontend environment variables
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies (or pyproject.toml)
â”œâ”€â”€ README.md # This file

```



---

## ğŸ§  Notes

- Multi-user ID system to personalize user experience.
- User sessions and chat history are stored both locally (browser) and in the backend.
- LLaMA 3 model must be downloaded via Ollama before the first run.

---

## ğŸ“¦ Dependencies (Core)

### Backend

- FastAPI  
- Langgraph  
- OpenAI  
- uvicorn  
- pydantic  
- ollama-python  

### Frontend

- React  
- Vite  
- TailwindCSS  
- FontAwesome  
- Google Fonts (via CDN)  

---

## ğŸ” Security

**Avoid using sensitive or personal data.**  
User identification is via localStorage and session IDs only. No authentication yet.

- Intended for local network use only, no external access.  
- CORS is set to allow all origins by default (`allow_origins=["*"]`). Restrict to known IPs for production security.

---

## ğŸ§ª Troubleshooting

- Minimal testing done; if making changes, watch out for potential infinite API call loops. Terminate if the app stops responding.  
- Adjust ports in backend/frontend if conflicts occur.  
- Ensure Ollama is installed and the `llama3:8b` model is downloaded. Test the llm alone in terminal to ensure it is responding.
- Check internet connectivity if styles or icons donâ€™t load (Google Fonts & FontAwesome use CDNs). These links are in `/frontend/index.html`.

---

## ğŸ“ Version v0.3.0 - Development Build

Planned Next:

- Add streaming to cut down user wait times
- Enable more abilities to `memory_node`
  - Fine tune `check_for_longterm_memory`
  - Add check for already stored memory for `check_for_longterm_memory`
  - Fine tune search for `get_context` for accuracy and latency

---

*Thank you for checking out Alfred! Feel free to contribute or report issues.*

