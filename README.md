# Alfred v0.1.3 - README

Alfred is a lightweight AI assistant project powered by a FastAPI backend, a React frontend, OpenAI GPT models for the main agent,  
and a LLaMA 3 local model running via Ollama for secondary agents.

---

## ğŸ› ï¸ Setup Instructions

Make sure the following components are installed before running Alfred:

- Python 3.10+
- Node.js (v18 or later recommended) on your global machine
- [Ollama](https://ollama.com)
- `uv` or `pip` for managing Python packages
- `npm` or `pnpm` for managing frontend packages

---

## ğŸ”‘ Environment Variables

Setup your environment variables:

1. Rename `.env.example` to `.env` in both the backend and the frontend directories.
   These `.env` files will hold all of your secrets (like API keys), which the scripts will load automatically when needed.

2. Gather the necessary API keys (see instructions and placeholders inside each `.env.example`) and paste them into your `.env` files.

3. Fill in any optional configuration details you want to personalize Alfred with, such as DEFAULT_USER_ID, LOCATION, and FAMILY_NAME.

---

## ğŸš€ Running the Project

Run these commands in separate terminals or background processes.

1. **Start the backend server:**

    ```bash
    source venv/bin/activate  # Activate your virtual environment
    uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    ```

    - `0.0.0.0` allows access from other devices on the same network (if firewall/network permits).
    - Change `--port` if 8000 is already in use, and update your frontend `.env` accordingly.

2. **Install frontend dependencies (only once or after updating package.json):**

    ```bash
    cd frontend
    npm install
    ```

    - This will generate the `node_modules` directory with the dependencies required for the frontend.

3. **Start the frontend:**

    ```bash
    cd frontend
    npm run dev --host
    ```

    - Verify you are running this from the `frontend` directory.

4. **Start the LLaMA 3 model with Ollama:**

    ```bash
    ollama run llama3:8b
    ```

    - You can run this from any terminal; it doesnâ€™t need to be inside the project folder or virtual environment.
    - Make sure it's running before using Alfred, as it currently powers the chat history and memory functionality.

5. **Access the frontend:**

    - On the server machine: [http://localhost:5173](http://localhost:5173)  
    - On another device in the local network: `http://<server-ip-address>:5173`

---

## ğŸ“ Directory Structure (partial)

alfred_v0.1.3/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ .venv # Python virtual environment (not committed)
â”‚ â”œâ”€â”€ nodes/ # Nodes (child agents)
â”‚ â”œâ”€â”€ utils/ # Tools and SQLite database utilities
â”‚ â”œâ”€â”€ .env # Backend environment variables
â”‚ â”œâ”€â”€ alfred_memory.db # Database (auto-generated)
â”‚ â”œâ”€â”€ main.py # FastAPI backend entry point
â”‚ â””â”€â”€ requirements.txt # Python dependencies
â”‚
â”œâ”€â”€ frontend/ # React frontend
â”‚ â”œâ”€â”€ public/
â”‚ â”œâ”€â”€ src/ # Main UI source (/components/Chat.jsx)
â”‚ â”œâ”€â”€ .env # Frontend environment variables
â”‚ â”œâ”€â”€ package.json
â”‚ â””â”€â”€ vite.config.js
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies (or pyproject.toml)
â”œâ”€â”€ README.md # This file


---

## ğŸ§  Notes

- Multi-user ID system to personalize user experience.
- User sessions and chat history are stored both locally (browser) and in the backend.
- LLaMA 3 model must be downloaded via Ollama before the first run.

---

## ğŸ“¦ Dependencies (Core)

### Backend

- FastAPI  
- Langgraph  
- OpenAI  
- uvicorn  
- pydantic  
- ollama-python  

### Frontend

- React  
- Vite  
- TailwindCSS  
- FontAwesome  
- Google Fonts (via CDN)  

---

## ğŸ” Security

**Avoid using sensitive or personal data.**  
User identification is via localStorage and session IDs only. No authentication yet.

- Intended for local network use only, no external access.  
- CORS is set to allow all origins by default (`allow_origins=["*"]`). Restrict to known IPs for production security.

---

## ğŸ§ª Troubleshooting

- Minimal testing done; if making changes, watch out for potential infinite API call loops. Terminate if the app stops responding.  
- Adjust ports in backend/frontend if conflicts occur.  
- Ensure Ollama is installed and the `llama3:8b` model is downloaded. Test the llm alone in terminal to insure it is responding.
- Check internet connectivity if styles or icons donâ€™t load (Google Fonts & FontAwesome use CDNs). These links are in `/frontend/index.html`.

---

## ğŸ“ Version v0.1.3 â€” Development Build

Features:

- Web UI chat interface  
- OpenAI API powered conversation  
- OpenWeather API integration for weather info and forecasts  
- Stores prompts and AI responses in SQLite with user/session IDs and timestamps  
- Local chat history memory recall (last 20 messages by default)  

Planned Next:

- Implement additional nodes (starting with memory node)  
- Merge prior memory_llama.py features into memory_node for supervisor agent
- Enable dynamic saving and recalling of longterm memories with LLaMA3 (memory node)
- Longterm memory stored in SQLite and referenced by Alfred for personalized queries

---

*Thank you for checking out Alfred! Feel free to contribute or report issues.*

